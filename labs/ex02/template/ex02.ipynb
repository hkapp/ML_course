{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cost Function\n",
    "Fill in the the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.067805854926391"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    \"\"\"MAE:\"\"\"\n",
    "    return (1 /y.shape[0]) * np.sum(np.abs(e))\n",
    "    \"\"\"\n",
    "    MSE:\n",
    "    return (1 /(2 * y.shape[0])) * e.T.dot(e)\n",
    "    \"\"\"\n",
    "\n",
    "compute_loss(y, tx, [-1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i,j] = compute_loss(y, tx, [w0[i], w1[j]])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=5.350259618178881, w0*=73.36683417085428, w1*=15.829145728643226, execution time=0.922 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "import datetime\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=200)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03960396 -0.35988335]\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    c = -1 / len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    \"\"\"MAE subgradient:\"\"\"\n",
    "    return c * tx.T.dot(np.sign(e))\n",
    "    \"\"\"\n",
    "    MSE gradient:\n",
    "    return c * tx.T.dot(e)\n",
    "    \"\"\"\n",
    "\n",
    "print(compute_gradient(y, tx, np.array([73, 13])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        print(gradient)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.00000000e+00  -8.72789190e-16]\n",
      "Gradient Descent(0/49): loss=74.06780585492638, w0=10.0, w1=1.0000000000000087\n",
      "[ -1.00000000e+00  -8.72789190e-16]\n",
      "Gradient Descent(1/49): loss=64.06780585492639, w0=20.0, w1=1.0000000000000173\n",
      "[ -1.00000000e+00  -8.72789190e-16]\n",
      "Gradient Descent(2/49): loss=54.06780585492637, w0=30.0, w1=1.000000000000026\n",
      "[ -1.00000000e+00  -8.72789190e-16]\n",
      "Gradient Descent(3/49): loss=44.06780585492638, w0=40.0, w1=1.0000000000000346\n",
      "[ -1.00000000e+00  -8.72789190e-16]\n",
      "Gradient Descent(4/49): loss=34.06780585492638, w0=50.0, w1=1.0000000000000433\n",
      "[-0.96039604 -0.05279159]\n",
      "Gradient Descent(5/49): loss=24.10684024142588, w0=59.603960396039604, w1=1.5279159082749174\n",
      "[-0.56435644 -0.42123203]\n",
      "Gradient Descent(6/49): loss=16.1852019076765, w0=65.24752475247524, w1=5.740236175158014\n",
      "[-0.3960396  -0.49721844]\n",
      "Gradient Descent(7/49): loss=11.473162608658766, w0=69.20792079207921, w1=10.712420563337902\n",
      "[-0.28712871 -0.46971169]\n",
      "Gradient Descent(8/49): loss=7.599521218256737, w0=72.07920792079207, w1=15.409537475822766\n",
      "[-0.10891089 -0.07923574]\n",
      "Gradient Descent(9/49): loss=5.36277280963621, w0=73.16831683168316, w1=16.201894895894974\n",
      "[ 0.04950495  0.026268  ]\n",
      "Gradient Descent(10/49): loss=5.332996252838113, w0=72.67326732673266, w1=15.939214857853244\n",
      "[ 0.00990099  0.00303729]\n",
      "Gradient Descent(11/49): loss=5.310906155245083, w0=72.57425742574256, w1=15.908842003771285\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(12/49): loss=5.312057134296903, w0=72.77227722772275, w1=15.988281708723003\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(13/49): loss=5.312431536509554, w0=72.57425742574256, w1=15.97349218093252\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(14/49): loss=5.31169824408637, w0=72.87128712871285, w1=16.02395967656667\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(15/49): loss=5.315168597048545, w0=72.57425742574256, w1=15.934247804495831\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(16/49): loss=5.311896300626017, w0=72.87128712871285, w1=15.984715300129977\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(17/49): loss=5.314816528400708, w0=72.57425742574256, w1=15.89500342805914\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(18/49): loss=5.312167067534055, w0=72.77227722772275, w1=15.974443133010856\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(19/49): loss=5.312411069909547, w0=72.57425742574256, w1=15.959653605220375\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(20/49): loss=5.311768083912304, w0=72.87128712871285, w1=16.010121100854523\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(21/49): loss=5.3150444485951525, w0=72.57425742574256, w1=15.920409228783685\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(22/49): loss=5.311966140451952, w0=72.87128712871285, w1=15.970876724417831\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(23/49): loss=5.314692379947314, w0=72.57425742574256, w1=15.881164852346993\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(24/49): loss=5.312277000771208, w0=72.77227722772275, w1=15.96060455729871\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(25/49): loss=5.312390603309539, w0=72.57425742574256, w1=15.945815029508228\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(26/49): loss=5.311837923738238, w0=72.87128712871285, w1=15.996282525142375\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(27/49): loss=5.31492030014176, w0=72.57425742574256, w1=15.906570653071537\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(28/49): loss=5.312075177839845, w0=72.77227722772275, w1=15.986010358023254\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(29/49): loss=5.312428177289125, w0=72.57425742574256, w1=15.971220830232772\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(30/49): loss=5.311709707024521, w0=72.87128712871285, w1=16.02168832586692\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(31/49): loss=5.315148220336205, w0=72.57425742574256, w1=15.93197645379608\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(32/49): loss=5.31190776356417, w0=72.87128712871285, w1=15.982443949430227\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(33/49): loss=5.314796151688368, w0=72.57425742574256, w1=15.892732077359389\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(34/49): loss=5.312185111076999, w0=72.77227722772275, w1=15.972171782311106\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(35/49): loss=5.312407710689117, w0=72.57425742574256, w1=15.957382254520624\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(36/49): loss=5.311779546850456, w0=72.87128712871285, w1=16.007849750154772\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(37/49): loss=5.315024071882813, w0=72.57425742574256, w1=15.918137878083934\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(38/49): loss=5.311983288145637, w0=72.77227722772275, w1=15.997577583035651\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(39/49): loss=5.312445284668703, w0=72.57425742574256, w1=15.98278805524517\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(40/49): loss=5.3116513301367405, w0=72.87128712871285, w1=16.033255550879318\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(41/49): loss=5.315251992077258, w0=72.57425742574256, w1=15.94354367880848\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(42/49): loss=5.311849386676389, w0=72.87128712871285, w1=15.994011174442626\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(43/49): loss=5.314899923429419, w0=72.57425742574256, w1=15.904299302371788\n",
      "[-0.01980198 -0.00794397]\n",
      "Gradient Descent(44/49): loss=5.3120932213827885, w0=72.77227722772275, w1=15.983739007323505\n",
      "[ 0.01980198  0.00147895]\n",
      "Gradient Descent(45/49): loss=5.3124248180686955, w0=72.57425742574256, w1=15.968949479533023\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(46/49): loss=5.311721169962674, w0=72.87128712871285, w1=16.01941697516717\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(47/49): loss=5.3151278436238645, w0=72.57425742574256, w1=15.929705103096333\n",
      "[-0.02970297 -0.00504675]\n",
      "Gradient Descent(48/49): loss=5.311919226502322, w0=72.87128712871285, w1=15.98017259873048\n",
      "[ 0.02970297  0.00897119]\n",
      "Gradient Descent(49/49): loss=5.3147757749760265, w0=72.57425742574256, w1=15.890460726659642\n",
      "Gradient Descent: execution time=0.026 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 1.0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14.07088101  10.9054746 ]\n"
     ]
    }
   ],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    idx = np.random.randint(0, N)\n",
    "    g = tx[idx] * (tx[idx].dot(w) - y[idx])\n",
    "    return g\n",
    "\n",
    "print(compute_stoch_gradient(y,tx,[90, 10]))\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_epochs, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_epochs):\n",
    "        rgrads = [compute_stoch_gradient(y, tx, w) for i in range(batch_size)]\n",
    "        gradient = np.mean(rgrads, axis = 0)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "        \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=279223671275.91675, w0=28.186148378569506, w1=-6.82471915841589\n",
      "Gradient Descent(1/49): loss=123887647967.0529, w0=52.75091747837005, w1=17.958303336292737\n",
      "Gradient Descent(2/49): loss=23642229353.05508, w0=59.80977315542564, w1=16.356617540729776\n",
      "Gradient Descent(3/49): loss=11043531442.225271, w0=63.73036424913114, w1=18.42565343445732\n",
      "Gradient Descent(4/49): loss=7334787250.117506, w0=66.78391286468522, w1=20.389208661757014\n",
      "Gradient Descent(5/49): loss=6044656640.733556, w0=63.86132695037122, w1=15.997648002148441\n",
      "Gradient Descent(6/49): loss=6304281233.401065, w0=69.28478394758442, w1=16.53093607499316\n",
      "Gradient Descent(7/49): loss=2807746468.9592695, w0=70.63836738470668, w1=16.31328885516076\n",
      "Gradient Descent(8/49): loss=2292645069.629924, w0=72.66309466596572, w1=14.957877722099411\n",
      "Gradient Descent(9/49): loss=1667734574.0848844, w0=73.21467865639988, w1=13.959579947630012\n",
      "Gradient Descent(10/49): loss=1550416403.75928, w0=71.39940744695953, w1=12.505933583480836\n",
      "Gradient Descent(11/49): loss=1765460319.4481106, w0=71.579260143907, w1=13.309360024964114\n",
      "Gradient Descent(12/49): loss=1687043048.4609873, w0=72.85312394437884, w1=13.287900431191018\n",
      "Gradient Descent(13/49): loss=1550143525.507757, w0=74.228527411553, w1=13.099229147186547\n",
      "Gradient Descent(14/49): loss=1589501527.0662458, w0=73.72359439085642, w1=11.738154920428212\n",
      "Gradient Descent(15/49): loss=1699470833.7918658, w0=73.87163820639043, w1=11.280147214529645\n",
      "Gradient Descent(16/49): loss=1797180945.4703584, w0=73.73764059085536, w1=11.526974930252466\n",
      "Gradient Descent(17/49): loss=1739092284.3033051, w0=71.96777135393276, w1=11.249191289999661\n",
      "Gradient Descent(18/49): loss=1875283792.877586, w0=70.41559177031405, w1=11.778312382576306\n",
      "Gradient Descent(19/49): loss=2097566139.9625847, w0=73.22776337104283, w1=11.754394959970062\n",
      "Gradient Descent(20/49): loss=1687643654.5864367, w0=73.40714983733358, w1=11.856373455005363\n",
      "Gradient Descent(21/49): loss=1670991286.2131886, w0=74.64601575951785, w1=12.94499805111399\n",
      "Gradient Descent(22/49): loss=1644292636.9407985, w0=73.6977532806311, w1=13.083144435625515\n",
      "Gradient Descent(23/49): loss=1554606080.8646934, w0=74.54748412071866, w1=11.113485274292069\n",
      "Gradient Descent(24/49): loss=1897111234.94508, w0=73.29108109771859, w1=11.582176750046\n",
      "Gradient Descent(25/49): loss=1718621274.2014413, w0=74.07918698315935, w1=12.3593719740325\n",
      "Gradient Descent(26/49): loss=1632178978.8292558, w0=76.66735721002595, w1=14.119721732074632\n",
      "Gradient Descent(27/49): loss=2128072637.0027096, w0=75.0539596385909, w1=14.246636729970914\n",
      "Gradient Descent(28/49): loss=1722884054.6869214, w0=73.95384671098209, w1=14.176004828922752\n",
      "Gradient Descent(29/49): loss=1584604972.8447497, w0=75.6860381231821, w1=14.727097780186135\n",
      "Gradient Descent(30/49): loss=1902498273.6893647, w0=74.2520555549756, w1=13.731292456768774\n",
      "Gradient Descent(31/49): loss=1587654407.507678, w0=74.32952191819653, w1=14.438947170815261\n",
      "Gradient Descent(32/49): loss=1638218710.1141393, w0=74.39048432324098, w1=15.692490175451534\n",
      "Gradient Descent(33/49): loss=1843530499.5239887, w0=75.33526840201444, w1=13.369417436229487\n",
      "Gradient Descent(34/49): loss=1747551792.4416304, w0=73.37921085643308, w1=11.016731207684291\n",
      "Gradient Descent(35/49): loss=1842266322.6193504, w0=74.41388107497495, w1=11.847925416166877\n",
      "Gradient Descent(36/49): loss=1734440646.8679328, w0=72.2274329241986, w1=13.048088336814136\n",
      "Gradient Descent(37/49): loss=1604773702.653908, w0=71.21026012066885, w1=13.96825244483748\n",
      "Gradient Descent(38/49): loss=1767604695.7516165, w0=71.64451413469742, w1=13.203774415862885\n",
      "Gradient Descent(39/49): loss=1678423192.0562365, w0=70.88998217388809, w1=13.94419115010292\n",
      "Gradient Descent(40/49): loss=1838322145.6070557, w0=72.299762471785, w1=13.407466993051075\n",
      "Gradient Descent(41/49): loss=1588267415.6633031, w0=71.50519455776436, w1=11.812518798750673\n",
      "Gradient Descent(42/49): loss=1837542811.4255311, w0=72.62220860340251, w1=11.806189587644493\n",
      "Gradient Descent(43/49): loss=1701182667.4119866, w0=73.17285695411819, w1=12.37243176502229\n",
      "Gradient Descent(44/49): loss=1600625148.2792463, w0=72.50111339008146, w1=14.52338854240732\n",
      "Gradient Descent(45/49): loss=1624479052.5076778, w0=71.6839981316783, w1=12.52589633533811\n",
      "Gradient Descent(46/49): loss=1713669787.9091222, w0=73.32855999690312, w1=10.503625396136394\n",
      "Gradient Descent(47/49): loss=1981503479.5584593, w0=69.81255137186206, w1=12.137968033311298\n",
      "Gradient Descent(48/49): loss=2234599762.1105924, w0=70.95445140739464, w1=10.894437844168444\n",
      "Gradient Descent(49/49): loss=2146427155.5558376, w0=71.05962584341985, w1=12.377097202443412\n",
      "SGD: execution time=0.024 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.4\n",
    "batch_size = 4\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0689c279525443bdbecfb209d5d66167": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "e57b1fc171f14d4d8ce90185b8f3937a": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
